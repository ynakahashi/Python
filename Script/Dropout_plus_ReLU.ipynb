{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Implementaion Dropout + ReLU(Rectifier Linear Unit)\n",
    "import sys\n",
    "import numpy as np\n",
    "np.seterr(all=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define functions\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def dsigmoid(x):\n",
    "    return x * (1. - x)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def dtanh(x):\n",
    "    return 1. - x * x\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x))\n",
    "    if e.ndim == 1:\n",
    "        return e / np.sum(e, axis=1)\n",
    "    else:\n",
    "        return e / np.array([np.sum(e, axis=1)]).T\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "def dReLU(x):\n",
    "    return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## dropout\n",
    "class Dropout(object):\n",
    "    def __init__(self, input, label, n_in, hidden_layer_sizes, n_out, rng=None, activation=ReLU):\n",
    "        self.x = input\n",
    "        self.y = label\n",
    "        self.hidden_layers = []\n",
    "        self.n_layers = len(hidden_layer_sizes)\n",
    "        if rng == None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "        assert self.n_layers > 0 # raise error if n_layers less than 0\n",
    "        \n",
    "        # construct multi-layer\n",
    "        for i in xrange(self.n_layers):\n",
    "            \n",
    "            # layer size\n",
    "            if i == 0:\n",
    "                input_size = n_in # n_in means the number of features(input layer)\n",
    "            else:\n",
    "                input_size = hidden_layer_sizes[i-1]\n",
    "\n",
    "            # layer input\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.hidden_layers[-1].output()    \n",
    "'''\n",
    "Although hidden_layers object doesn't exist at this moment, \n",
    "it's OK because the 1st roop(i==0) always assignes self.x.\n",
    "The following procecss generates hidden_layer object.\n",
    "'''\n",
    "                                                            \n",
    "            # construct hidden layer\n",
    "            hidden_layer = HiddenLayer(input=layer_input,\n",
    "                                       n_in=input_size,\n",
    "                                       n_out=hidden_layer_sizes[i],\n",
    "                                       rng=rng,\n",
    "                                       activation=activation)\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            \n",
    "            # layer for output using Logistic regression(softmax)\n",
    "            self.log_layer = LogisticRegression(input=self.hidden_layers[-1].output(),\n",
    "                                                label=self.y,\n",
    "                                                n_in=hidden_layer_sizes[-1],\n",
    "                                                n_out=n_out)\n",
    "\n",
    "    def train(self, epochs=5000, dropout=True, p_dropout=0.5, rng=None):\n",
    "        for epoch in xrange(epochs):\n",
    "            dropout_masks = []\n",
    "            \n",
    "            # forward hidden layers\n",
    "            for i in xrange(self.n_layers):\n",
    "                if i == 0:\n",
    "                    layer_input = self.x\n",
    "                layer_input = self.hidden_layers[i].forward(input=layer_input)\n",
    "                if dropout == True:\n",
    "                    mask = self.hidden_layers[i].dropout(input=layer_input, p=p_dropout, rng=rng)\n",
    "                    layer_input *= mask\n",
    "                    dropout_masks.append(mask)\n",
    "                    \n",
    "            # forward & backward log layer\n",
    "            self.log_layer.train(input=layer_input)\n",
    "            \n",
    "            # backward hidden layers\n",
    "            for i in reversed(xrange(0, self.n_layers)):\n",
    "                if i == self.n_layers-1:\n",
    "                    prev_layer = self.log_layer\n",
    "                else:\n",
    "                    prev_layer = self.hidden_layers[i+1]\n",
    "                \n",
    "                self.hidden_layers[i].backward(prev_layer=prev_layer)\n",
    "                \n",
    "                if dropout == True:\n",
    "                    self.hidden_layers[i].d_y *= dropout_masks[i]\n",
    "\n",
    "    def predict(self, x, dropout=True, p_dropout=0.5):\n",
    "        layer_input = x\n",
    "\n",
    "        for i in xrange(self.n_layers):\n",
    "            if dropout == True:\n",
    "                self.hidden_layers[i].W = p_dropout * self.hidden_layers[i].W\n",
    "                self.hidden_layers[i].b = p_dropout * self.hidden_layers[i].b\n",
    "            \n",
    "            layer_input = self.hidden_layers[i].output(input=layer_input)\n",
    "\n",
    "        return self.log_layer.predict(layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None, rng=None, activation=tanh):\n",
    "        \n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "\n",
    "        if W is None:\n",
    "            a = 1. / n_in\n",
    "            W = np.array(rng.uniform(low=-a,\n",
    "                                     high=a,\n",
    "                                     size=(n_in, n_out)))  # initialize W uniformly\n",
    "\n",
    "        if b is None:\n",
    "            b = np.zeros(n_out)  # initialize bias 0\n",
    "\n",
    "        self.rng = rng\n",
    "        self.x = input\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        if activation == tanh:\n",
    "            self.dactivation = dtanh\n",
    "        elif activation == sigmoid:\n",
    "            self.dactivation = dsigmoid\n",
    "        elif activation == ReLU:\n",
    "            self.dactivation = dReLU\n",
    "        else:\n",
    "            raise ValueError('activation function not supported.')\n",
    "\n",
    "        self.activation = activation\n",
    "        \n",
    "    def output(self, input=None):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "        \n",
    "        linear_output = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return (linear_output if self.activation is None\n",
    "                else self.activation(linear_output))\n",
    "\n",
    "    def sample_h_given_v(self, input=None):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        v_mean = self.output()\n",
    "        h_sample = self.rng.binomial(size=v_mean.shape,\n",
    "                                     n=1,\n",
    "                                     p=v_mean)\n",
    "        return h_sample\n",
    "\n",
    "    def forward(self, input=None):\n",
    "        return self.output(input=input)\n",
    "\n",
    "    def backward(self, prev_layer, lr=0.1, input=None):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        d_y = self.dactivation(prev_layer.x) * np.dot(prev_layer.d_y, prev_layer.W.T)\n",
    "\n",
    "        self.W += lr * np.dot(self.x.T, d_y)\n",
    "        self.b += lr * np.mean(d_y, axis=0)\n",
    "\n",
    "        self.d_y = d_y\n",
    "\n",
    "    def dropout(self, input, p, rng=None):\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(123)\n",
    "\n",
    "        mask = rng.binomial(size=input.shape,\n",
    "                            n=1,\n",
    "                            p=1-p)  # p is the prob of dropping\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, label, n_in, n_out):\n",
    "        self.x = input\n",
    "        self.y = label\n",
    "        self.W = np.zeros((n_in, n_out))  # initialize W 0\n",
    "        self.b = np.zeros(n_out)          # initialize bias 0\n",
    "\n",
    "\n",
    "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        p_y_given_x = softmax(np.dot(self.x, self.W) + self.b)\n",
    "        d_y = self.y - p_y_given_x\n",
    "        \n",
    "        # Weight update\n",
    "        self.W += lr * np.dot(self.x.T, d_y) - lr * L2_reg * self.W\n",
    "        self.b += lr * np.mean(d_y, axis=0)\n",
    "\n",
    "        self.d_y = d_y\n",
    "\n",
    "        # cost = self.negative_log_likelihood()\n",
    "        # return cost\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        sigmoid_activation = softmax(np.dot(self.x, self.W) + self.b)\n",
    "\n",
    "        cross_entropy = - np.mean(\n",
    "            np.sum(self.y * np.log(sigmoid_activation) +\n",
    "            (1 - self.y) * np.log(1 - sigmoid_activation), axis=1))\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return softmax(np.dot(x, self.W) + self.b)\n",
    "\n",
    "    def output(self, x):\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_dropout(n_epochs=5000, dropout=True, p_dropout=0.5):\n",
    "\n",
    "    # XOR\n",
    "    x = np.array([[0,  0],\n",
    "                  [0,  1],\n",
    "                  [1,  0],\n",
    "                  [1,  1]])\n",
    "\n",
    "    y = np.array([[0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 0],\n",
    "                  [0, 1]])\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "\n",
    "    # construct Dropout MLP\n",
    "    classifier = Dropout(input=x, label=y, n_in=2, hidden_layer_sizes=[10, 10], n_out=2,\n",
    "                         rng=rng, activation=ReLU)\n",
    "\n",
    "    # train\n",
    "    classifier.train(epochs=n_epochs, dropout=dropout, p_dropout=p_dropout, rng=rng)\n",
    "\n",
    "    # test\n",
    "    print classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.52481527e-02   9.44751847e-01]\n",
      " [  1.00000000e+00   2.82725901e-20]\n",
      " [  1.00000000e+00   1.90429882e-21]\n",
      " [  5.52483354e-02   9.44751665e-01]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "xxx = [10, 10]\n",
    "xlen = len(xxx)\n",
    "for i in xrange(xlen):\n",
    "    print i\n",
    "for i in range(xlen):\n",
    "    print i\n",
    "\n",
    "#print range(xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
