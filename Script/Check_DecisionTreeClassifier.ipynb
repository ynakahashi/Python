{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):\n",
      "    \"\"\"A decision tree classifier.\n",
      "\n",
      "    Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    criterion : string, optional (default=\"gini\")\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "\n",
      "    splitter : string, optional (default=\"best\")\n",
      "        The strategy used to choose the split at each node. Supported\n",
      "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "        the best random split.\n",
      "\n",
      "    max_depth : int or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a percentage and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node:\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a percentage and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=None)\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "            - If int, then consider `max_features` features at each split.\n",
      "            - If float, then `max_features` is a percentage and\n",
      "              `int(max_features * n_features)` features are considered at each\n",
      "              split.\n",
      "            - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"log2\", then `max_features=log2(n_features)`.\n",
      "            - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    min_impurity_split : float,\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If not given, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    presort : bool, optional (default=False)\n",
      "        Whether to presort the data to speed up the finding of best splits in\n",
      "        fitting. For the default settings of a decision tree on large\n",
      "        datasets, setting this to true may slow down the training process.\n",
      "        When using either a smaller dataset or a restricted depth, this may\n",
      "        speed up the training.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "        The classes labels (single output problem),\n",
      "        or a list of arrays of class labels (multi-output problem).\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances. The higher, the more important the\n",
      "        feature. The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance [4]_.\n",
      "\n",
      "    max_features_ : int,\n",
      "        The inferred value of max_features.\n",
      "\n",
      "    n_classes_ : int or list\n",
      "        The number of classes (for single output problems),\n",
      "        or a list containing the number of classes for each\n",
      "        output (for multi-output problems).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    tree_ : Tree object\n",
      "        The underlying Tree object.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data and\n",
      "    ``max_features=n_features``, if the improvement of the criterion is\n",
      "    identical for several splits enumerated during the search of the best\n",
      "    split. To obtain a deterministic behaviour during fitting,\n",
      "    ``random_state`` has to be fixed.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "           Learning\", Springer, 2009.\n",
      "\n",
      "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "           http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> from sklearn.tree import DecisionTreeClassifier\n",
      "    >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "    >>> iris = load_iris()\n",
      "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "    ...                             # doctest: +SKIP\n",
      "    ...\n",
      "    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 criterion=\"gini\",\n",
      "                 splitter=\"best\",\n",
      "                 max_depth=None,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 min_weight_fraction_leaf=0.,\n",
      "                 max_features=None,\n",
      "                 random_state=None,\n",
      "                 max_leaf_nodes=None,\n",
      "                 min_impurity_decrease=0.,\n",
      "                 min_impurity_split=None,\n",
      "                 class_weight=None,\n",
      "                 presort=False):\n",
      "        super(DecisionTreeClassifier, self).__init__(\n",
      "            criterion=criterion,\n",
      "            splitter=splitter,\n",
      "            max_depth=max_depth,\n",
      "            min_samples_split=min_samples_split,\n",
      "            min_samples_leaf=min_samples_leaf,\n",
      "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
      "            max_features=max_features,\n",
      "            max_leaf_nodes=max_leaf_nodes,\n",
      "            class_weight=class_weight,\n",
      "            random_state=random_state,\n",
      "            min_impurity_decrease=min_impurity_decrease,\n",
      "            min_impurity_split=min_impurity_split,\n",
      "            presort=presort)\n",
      "\n",
      "    def fit(self, X, y, sample_weight=None, check_input=True,\n",
      "            X_idx_sorted=None):\n",
      "        \"\"\"Build a decision tree classifier from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "            The training input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csc_matrix``.\n",
      "\n",
      "        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "            The target values (class labels) as integers or strings.\n",
      "\n",
      "        sample_weight : array-like, shape = [n_samples] or None\n",
      "            Sample weights. If None, then samples are equally weighted. Splits\n",
      "            that would create child nodes with net zero or negative weight are\n",
      "            ignored while searching for a split in each node. Splits are also\n",
      "            ignored if they would result in any single class carrying a\n",
      "            negative weight in either child node.\n",
      "\n",
      "        check_input : boolean, (default=True)\n",
      "            Allow to bypass several input checking.\n",
      "            Don't use this parameter unless you know what you do.\n",
      "\n",
      "        X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      "            The indexes of the sorted training input samples. If many tree\n",
      "            are grown on the same dataset, this allows the ordering to be\n",
      "            cached between trees. If None, the data will be sorted here.\n",
      "            Don't use this parameter unless you know what to do.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Returns self.\n",
      "        \"\"\"\n",
      "\n",
      "        super(DecisionTreeClassifier, self).fit(\n",
      "            X, y,\n",
      "            sample_weight=sample_weight,\n",
      "            check_input=check_input,\n",
      "            X_idx_sorted=X_idx_sorted)\n",
      "        return self\n",
      "\n",
      "    def predict_proba(self, X, check_input=True):\n",
      "        \"\"\"Predict class probabilities of the input samples X.\n",
      "\n",
      "        The predicted class probability is the fraction of samples of the same\n",
      "        class in a leaf.\n",
      "\n",
      "        check_input : boolean, (default=True)\n",
      "            Allow to bypass several input checking.\n",
      "            Don't use this parameter unless you know what you do.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "            The input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csr_matrix``.\n",
      "\n",
      "        check_input : bool\n",
      "            Run check_array on X.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "            such arrays if n_outputs > 1.\n",
      "            The class probabilities of the input samples. The order of the\n",
      "            classes corresponds to that in the attribute `classes_`.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self, 'tree_')\n",
      "        X = self._validate_X_predict(X, check_input)\n",
      "        proba = self.tree_.predict(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            proba = proba[:, :self.n_classes_]\n",
      "            normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
      "            normalizer[normalizer == 0.0] = 1.0\n",
      "            proba /= normalizer\n",
      "\n",
      "            return proba\n",
      "\n",
      "        else:\n",
      "            all_proba = []\n",
      "\n",
      "            for k in range(self.n_outputs_):\n",
      "                proba_k = proba[:, k, :self.n_classes_[k]]\n",
      "                normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
      "                normalizer[normalizer == 0.0] = 1.0\n",
      "                proba_k /= normalizer\n",
      "                all_proba.append(proba_k)\n",
      "\n",
      "            return all_proba\n",
      "\n",
      "    def predict_log_proba(self, X):\n",
      "        \"\"\"Predict class log-probabilities of the input samples X.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "            The input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csr_matrix``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "            such arrays if n_outputs > 1.\n",
      "            The class log-probabilities of the input samples. The order of the\n",
      "            classes corresponds to that in the attribute `classes_`.\n",
      "        \"\"\"\n",
      "        proba = self.predict_proba(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            return np.log(proba)\n",
      "\n",
      "        else:\n",
      "            for k in range(self.n_outputs_):\n",
      "                proba[k] = np.log(proba[k])\n",
      "\n",
      "            return proba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(DecisionTreeClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfile(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n"
     ]
    }
   ],
   "source": [
    "print(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
